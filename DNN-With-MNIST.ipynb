{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#reset tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Import Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "#Training & Test Data\n",
    "trainX = mnist.train.images\n",
    "trainY = mnist.train.labels\n",
    "\n",
    "testX = mnist.test.images\n",
    "testY = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classifier Parameters\n",
    "logs_path='/tmp/session4/v2'\n",
    "save_path=logs_path + '/'\n",
    "learning_rate = 0.003 #0.003, 0.0001, 0.0002\n",
    "n_features = 784\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "training_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 200\n",
    "L = 100\n",
    "M = 60\n",
    "N = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build Graph - Define Tensorflow Operations\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    # None -> batch size can be any size, with n_features\n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_features], name=\"x-input\") \n",
    "    # target n_classes output classes\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y-input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize a layer 1 of 200 neurons\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    W1 = tf.Variable(tf.truncated_normal([n_features, K] ,stddev=0.1))\n",
    "    b1 = tf.Variable(tf.ones([K]))\n",
    "    Y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize a layer 2 of 100 neurons\n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.truncated_normal([K, L] ,stddev=0.1))\n",
    "    b2 = tf.Variable(tf.ones([L]))\n",
    "    Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize a layer 3 of 60 neurons\n",
    "with tf.name_scope(\"Layer3\"):\n",
    "    W3 = tf.Variable(tf.truncated_normal([L, M] ,stddev=0.1))\n",
    "    b3 = tf.Variable(tf.ones([M]))\n",
    "    Y3 = tf.nn.sigmoid(tf.matmul(Y2, W3) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize a layer 4 of 30 neurons\n",
    "with tf.name_scope(\"Layer4\"):\n",
    "    W4 = tf.Variable(tf.truncated_normal([M, N] ,stddev=0.1))\n",
    "    b4 = tf.Variable(tf.ones([N]))\n",
    "    Y4 = tf.nn.sigmoid(tf.matmul(Y3, W4) + b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement model\n",
    "with tf.name_scope(\"Output\"):\n",
    "    # y is our prediction\n",
    "    W = tf.Variable(tf.truncated_normal([N, n_classes] ,stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros([n_classes]))   \n",
    "    #y = tf.nn.softmax(tf.matmul(Y4, W) + b)\n",
    "    Ylogits = tf.matmul(Y4, W) + b\n",
    "    y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify cost function\n",
    "with tf.name_scope('Loss'):\n",
    "    # this is our cost\n",
    "    #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=y_)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify optimizer\n",
    "with tf.name_scope('train'):\n",
    "    # optimizer is an \"operation\" which we can execute in a session\n",
    "    learn_rate = tf.Variable(learning_rate)\n",
    "    train_op = tf.train.GradientDescentOptimizer(learn_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Accuracy'):\n",
    "    # Prediction\n",
    "    prediction = tf.argmax(y,1,name=\"Predict\")\n",
    "    #Accuracy\n",
    "    correct_prediction = tf.equal(prediction, tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a summary for our cost and accuracy\n",
    "training_loss = tf.summary.scalar(\"training_loss\", cross_entropy)\n",
    "training_accuracy = tf.summary.scalar(\"training_accuracy\", accuracy)\n",
    "test_loss = tf.summary.scalar(\"test_loss\", cross_entropy)\n",
    "test_accuracy = tf.summary.scalar(\"test_accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a Saver to save the graph\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss at step:  0  is  230.172\n",
      "Epoch:  0\n",
      "Test loss at step:  1  is  230.096\n",
      "Test loss at step:  2  is  229.929\n",
      "Test loss at step:  3  is  222.904\n",
      "Test loss at step:  4  is  160.62\n",
      "Test loss at step:  5  is  117.283\n",
      "Epoch:  5\n",
      "Test loss at step:  6  is  80.6451\n",
      "Test loss at step:  7  is  58.8372\n",
      "Test loss at step:  8  is  47.7768\n",
      "Test loss at step:  9  is  39.9217\n",
      "Test loss at step:  10  is  33.2838\n",
      "Epoch:  10\n",
      "Test loss at step:  11  is  28.1963\n",
      "Test loss at step:  12  is  24.8707\n",
      "Test loss at step:  13  is  22.455\n",
      "Test loss at step:  14  is  20.4755\n",
      "Test loss at step:  15  is  18.849\n",
      "Epoch:  15\n",
      "Test loss at step:  16  is  17.5824\n",
      "Test loss at step:  17  is  16.6293\n",
      "Test loss at step:  18  is  15.8968\n",
      "Test loss at step:  19  is  15.2794\n",
      "Test loss at step:  20  is  14.7145\n",
      "Epoch:  20\n",
      "Test loss at step:  21  is  14.1965\n",
      "Test loss at step:  22  is  13.7388\n",
      "Test loss at step:  23  is  13.3216\n",
      "Test loss at step:  24  is  12.9344\n",
      "Test loss at step:  25  is  12.5913\n",
      "Epoch:  25\n",
      "Test loss at step:  26  is  12.3404\n",
      "Test loss at step:  27  is  12.1979\n",
      "Test loss at step:  28  is  12.1213\n",
      "Test loss at step:  29  is  12.0477\n",
      "Test loss at step:  30  is  11.9183\n",
      "Epoch:  30\n",
      "Test loss at step:  31  is  11.6891\n",
      "Test loss at step:  32  is  11.4682\n",
      "Test loss at step:  33  is  11.3834\n",
      "Test loss at step:  34  is  11.4092\n",
      "Test loss at step:  35  is  11.5356\n",
      "Epoch:  35\n",
      "Test loss at step:  36  is  11.7621\n",
      "Test loss at step:  37  is  12.0737\n",
      "Test loss at step:  38  is  12.1854\n",
      "Test loss at step:  39  is  12.1237\n",
      "Test loss at step:  40  is  12.1004\n",
      "Epoch:  40\n",
      "Test loss at step:  41  is  12.0805\n",
      "Test loss at step:  42  is  11.9972\n",
      "Test loss at step:  43  is  11.9191\n",
      "Test loss at step:  44  is  11.8719\n",
      "Test loss at step:  45  is  11.8675\n",
      "Epoch:  45\n",
      "Test loss at step:  46  is  11.9285\n",
      "Test loss at step:  47  is  11.9983\n",
      "Test loss at step:  48  is  12.0925\n",
      "Test loss at step:  49  is  12.1877\n",
      "Test loss at step:  50  is  12.2891\n",
      "Epoch:  50\n",
      "Test loss at step:  51  is  12.3836\n",
      "Test loss at step:  52  is  12.465\n",
      "Test loss at step:  53  is  12.5423\n",
      "Test loss at step:  54  is  12.6027\n",
      "Test loss at step:  55  is  12.6632\n",
      "Epoch:  55\n",
      "Test loss at step:  56  is  12.7444\n",
      "Test loss at step:  57  is  12.8717\n",
      "Test loss at step:  58  is  13.0778\n",
      "Test loss at step:  59  is  13.4348\n",
      "Test loss at step:  60  is  14.118\n",
      "Epoch:  60\n",
      "Test loss at step:  61  is  15.3906\n",
      "Test loss at step:  62  is  16.9748\n",
      "Test loss at step:  63  is  15.2623\n",
      "Test loss at step:  64  is  13.4829\n",
      "Test loss at step:  65  is  12.8946\n",
      "Epoch:  65\n",
      "Test loss at step:  66  is  12.7752\n",
      "Test loss at step:  67  is  12.7547\n",
      "Test loss at step:  68  is  12.7669\n",
      "Test loss at step:  69  is  12.784\n",
      "Test loss at step:  70  is  12.8037\n",
      "Epoch:  70\n",
      "Test loss at step:  71  is  12.8245\n",
      "Test loss at step:  72  is  12.8455\n",
      "Test loss at step:  73  is  12.8675\n",
      "Test loss at step:  74  is  12.8914\n",
      "Test loss at step:  75  is  12.9127\n",
      "Epoch:  75\n",
      "Test loss at step:  76  is  12.9401\n",
      "Test loss at step:  77  is  12.9709\n",
      "Test loss at step:  78  is  13.0021\n",
      "Test loss at step:  79  is  13.0331\n",
      "Test loss at step:  80  is  13.064\n",
      "Epoch:  80\n",
      "Test loss at step:  81  is  13.0946\n",
      "Test loss at step:  82  is  13.1247\n",
      "Test loss at step:  83  is  13.1542\n",
      "Test loss at step:  84  is  13.183\n",
      "Test loss at step:  85  is  13.211\n",
      "Epoch:  85\n",
      "Test loss at step:  86  is  13.2382\n",
      "Test loss at step:  87  is  13.2648\n",
      "Test loss at step:  88  is  13.2908\n",
      "Test loss at step:  89  is  13.3162\n",
      "Test loss at step:  90  is  13.3413\n",
      "Epoch:  90\n",
      "Test loss at step:  91  is  13.3661\n",
      "Test loss at step:  92  is  13.3906\n",
      "Test loss at step:  93  is  13.415\n",
      "Test loss at step:  94  is  13.4394\n",
      "Test loss at step:  95  is  13.4639\n",
      "Epoch:  95\n",
      "Test loss at step:  96  is  13.4887\n",
      "Test loss at step:  97  is  13.5138\n",
      "Test loss at step:  98  is  13.5396\n",
      "Test loss at step:  99  is  13.5662\n",
      "Accuracy:  0.9739\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Start Graph execution\n",
    "with tf.Session() as sess:\n",
    "    # variables need to be initialized before we can use them\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # create log writer object\n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # perform training cycles\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # number of batches in one epoch\n",
    "        batch_count = int(trainX.shape[0]/batch_size)\n",
    "        \n",
    "        for i in range(batch_count):\n",
    "            batch_x  = trainX[i*batch_size:i*batch_size+batch_size]\n",
    "            batch_y  = trainY[i*batch_size:i*batch_size+batch_size]\n",
    "\n",
    "            # perform the operations we defined earlier on batch\n",
    "            _,acc,loss = sess.run([train_op, training_accuracy,training_loss], feed_dict={x: batch_x, y_: batch_y})\n",
    "            \n",
    "            #log training accuracy and loss\n",
    "            writer.add_summary(acc, epoch * batch_count + i)\n",
    "            writer.add_summary(loss, epoch * batch_count + i)    \n",
    "                        \n",
    "        #Test loss and accuracy\n",
    "        acc,loss,a_loss = sess.run([test_accuracy,test_loss,cross_entropy],\n",
    "                                   feed_dict={x: testX, y_: testY})\n",
    "        writer.add_summary(acc, epoch * batch_count + i)\n",
    "        writer.add_summary(loss, epoch * batch_count + i)\n",
    "        print ('Test loss at step: ', epoch, ' is ', a_loss) \n",
    "        if epoch % 5 == 0: \n",
    "            print (\"Epoch: \", epoch)\n",
    "                \n",
    "    print (\"Accuracy: \", accuracy.eval(feed_dict={x: testX, y_: testY}))\n",
    "    \n",
    "    #Save the model\n",
    "    saver.save(sess, save_path + \"model.ckpt\")\n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
